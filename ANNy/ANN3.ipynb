{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOR6y7sQmle7yYMsnM+S7J7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8jVwKmUYB4Gx","executionInfo":{"status":"ok","timestamp":1715101836854,"user_tz":-330,"elapsed":1303,"user":{"displayName":"Paras Deshpande","userId":"02962485204970653198"}},"outputId":"a8a51dcf-134d-46fb-a166-d1febd31d77c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.7668263997431923\n","Epoch 100, Loss: 0.7262300640720414\n","Epoch 200, Loss: 0.7081943486038822\n","Epoch 300, Loss: 0.7003451282006334\n","Epoch 400, Loss: 0.6969427817542326\n","Epoch 500, Loss: 0.695456314284528\n","Epoch 600, Loss: 0.6947917138491524\n","Epoch 700, Loss: 0.6944795011895517\n","Epoch 800, Loss: 0.6943186526838908\n","Epoch 900, Loss: 0.6942231244607253\n","Epoch 1000, Loss: 0.6941560738567456\n","Epoch 1100, Loss: 0.6941017109970968\n","Epoch 1200, Loss: 0.6940532556537914\n","Epoch 1300, Loss: 0.6940077839440554\n","Epoch 1400, Loss: 0.6939640229421588\n","Epoch 1500, Loss: 0.6939214068430929\n","Epoch 1600, Loss: 0.6938796730495345\n","Epoch 1700, Loss: 0.6938386893273897\n","Epoch 1800, Loss: 0.6937983798373258\n","Epoch 1900, Loss: 0.6937586934683411\n","Epoch 2000, Loss: 0.6937195902661888\n","Epoch 2100, Loss: 0.6936810356011768\n","Epoch 2200, Loss: 0.6936429976464598\n","Epoch 2300, Loss: 0.6936054462730499\n","Epoch 2400, Loss: 0.6935683525518357\n","Epoch 2500, Loss: 0.6935316885163934\n","Epoch 2600, Loss: 0.6934954270384937\n","Epoch 2700, Loss: 0.6934595417528915\n","Epoch 2800, Loss: 0.6934240070042064\n","Epoch 2900, Loss: 0.6933887978041748\n","Epoch 3000, Loss: 0.6933538897942081\n","Epoch 3100, Loss: 0.6933192592110141\n","Epoch 3200, Loss: 0.6932848828542868\n","Epoch 3300, Loss: 0.6932507380559813\n","Epoch 3400, Loss: 0.6932168026509361\n","Epoch 3500, Loss: 0.6931830549486975\n","Epoch 3600, Loss: 0.6931494737064562\n","Epoch 3700, Loss: 0.6931160381030226\n","Epoch 3800, Loss: 0.6930827277137832\n","Epoch 3900, Loss: 0.6930495224865832\n","Epoch 4000, Loss: 0.693016402718489\n","Epoch 4100, Loss: 0.6929833490333837\n","Epoch 4200, Loss: 0.6929503423603547\n","Epoch 4300, Loss: 0.6929173639128298\n","Epoch 4400, Loss: 0.6928843951684285\n","Epoch 4500, Loss: 0.692851417849486\n","Epoch 4600, Loss: 0.6928184139042195\n","Epoch 4700, Loss: 0.6927853654885022\n","Epoch 4800, Loss: 0.692752254948213\n","Epoch 4900, Loss: 0.6927190648021322\n","Epoch 5000, Loss: 0.6926857777253546\n","Epoch 5100, Loss: 0.6926523765331918\n","Epoch 5200, Loss: 0.6926188441655386\n","Epoch 5300, Loss: 0.6925851636716779\n","Epoch 5400, Loss: 0.6925513181955\n","Epoch 5500, Loss: 0.6925172909611137\n","Epoch 5600, Loss: 0.6924830652588303\n","Epoch 5700, Loss: 0.6924486244314931\n","Epoch 5800, Loss: 0.6924139518611405\n","Epoch 5900, Loss: 0.6923790309559784\n","Epoch 6000, Loss: 0.6923438451376458\n","Epoch 6100, Loss: 0.6923083778287571\n","Epoch 6200, Loss: 0.6922726124407038\n","Epoch 6300, Loss: 0.6922365323616986\n","Epoch 6400, Loss: 0.6922001209450512\n","Epoch 6500, Loss: 0.6921633614976567\n","Epoch 6600, Loss: 0.6921262372686854\n","Epoch 6700, Loss: 0.6920887314384621\n","Epoch 6800, Loss: 0.692050827107519\n","Epoch 6900, Loss: 0.6920125072858148\n","Epoch 7000, Loss: 0.6919737548821046\n","Epoch 7100, Loss: 0.691934552693452\n","Epoch 7200, Loss: 0.6918948833948745\n","Epoch 7300, Loss: 0.6918547295291072\n","Epoch 7400, Loss: 0.6918140734964827\n","Epoch 7500, Loss: 0.6917728975449111\n","Epoch 7600, Loss: 0.6917311837599576\n","Epoch 7700, Loss: 0.6916889140550044\n","Epoch 7800, Loss: 0.6916460701614938\n","Epoch 7900, Loss: 0.6916026336192416\n","Epoch 8000, Loss: 0.691558585766815\n","Epoch 8100, Loss: 0.6915139077319693\n","Epoch 8200, Loss: 0.6914685804221353\n","Epoch 8300, Loss: 0.6914225845149518\n","Epoch 8400, Loss: 0.6913759004488381\n","Epoch 8500, Loss: 0.691328508413601\n","Epoch 8600, Loss: 0.6912803883410691\n","Epoch 8700, Loss: 0.6912315198957513\n","Epoch 8800, Loss: 0.6911818824655147\n","Epoch 8900, Loss: 0.6911314551522771\n","Epoch 9000, Loss: 0.691080216762708\n","Epoch 9100, Loss: 0.691028145798937\n","Epoch 9200, Loss: 0.6909752204492626\n","Epoch 9300, Loss: 0.6909214185788616\n","Epoch 9400, Loss: 0.6908667177204904\n","Epoch 9500, Loss: 0.6908110950651791\n","Epoch 9600, Loss: 0.6907545274529142\n","Epoch 9700, Loss: 0.6906969913633045\n","Epoch 9800, Loss: 0.6906384629062302\n","Epoch 9900, Loss: 0.6905789178124712\n","Predictions: [[0. 0. 1. 1.]]\n"]}],"source":["import numpy as np\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","def initialize_parameters(input_size, hidden_size, output_size):\n","    W1 = np.random.uniform(-1, 1, (hidden_size, input_size))\n","    b1 = np.random.uniform(-1, 1, (hidden_size, 1))\n","    W2 = np.random.uniform(-1, 1, (output_size, hidden_size))\n","    b2 = np.random.uniform(-1, 1, (output_size, 1))\n","    return W1, b1, W2, b2\n","def forward_propagation(X, W1, b1, W2, b2):\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = sigmoid(Z1)\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = sigmoid(Z2)\n","    return A1, A2\n","def compute_loss(Y, A2):\n","    m = Y.shape[1]\n","    loss = -1/m * np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2))\n","    return loss\n","def backward_propagation(X, Y, A1, A2, W1, W2, b1, b2):\n","    m = Y.shape[1]\n","    dZ2 = A2 - Y\n","    dW2 = 1/m * np.dot(dZ2, A1.T)\n","    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n","    dZ1 = np.dot(W2.T, dZ2) * A1 * (1 - A1)\n","    dW1 = 1/m * np.dot(dZ1, X.T)\n","    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n","    return dW1, db1, dW2, db2\n","def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n","    W1 -= learning_rate * dW1\n","    b1 -= learning_rate * db1\n","    W2 -= learning_rate * dW2\n","    b2 -= learning_rate * db2\n","    return W1, b1, W2, b2\n","def train(X, Y, hidden_size, output_size, learning_rate, num_epochs):\n","    input_size = X.shape[0]\n","    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n","\n","    for epoch in range(num_epochs):\n","        A1, A2 = forward_propagation(X, W1, b1, W2, b2)\n","        loss = compute_loss(Y, A2)\n","        dW1, db1, dW2, db2 = backward_propagation(X, Y, A1, A2, W1, W2, b1, b2)\n","        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n","\n","        if epoch % 100 == 0:\n","            print(f\"Epoch {epoch}, Loss: {loss}\")\n","\n","    return W1, b1, W2, b2\n","def predict(X, W1, b1, W2, b2):\n","    _, A2 = forward_propagation(X, W1, b1, W2, b2)\n","    predictions = np.round(A2)\n","    return predictions\n","# Example usage\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n","Y = np.array([[0, 1, 1, 0]])\n","\n","hidden_size = 2\n","output_size = 1\n","learning_rate = 0.01\n","num_epochs = 10000\n","\n","W1, b1, W2, b2 = train(X, Y, hidden_size, output_size, learning_rate, num_epochs)\n","\n","# Make predictions\n","predictions = predict(X, W1, b1, W2, b2)\n","print(\"Predictions:\", predictions)"]}]}